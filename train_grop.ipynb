{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9bec36a",
   "metadata": {},
   "source": [
    "# Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7274de67",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install unsloth\n",
    "! pip install unsloth vllm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e18b776",
   "metadata": {},
   "source": [
    "# Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f951d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "\n",
    "max_seq_length = 4500\n",
    "lora_rank = 32\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"Qwen/Qwen3-4B-Base\",\n",
    "    max_seq_length = max_seq_length,   # Context length - can be longer, but uses more memory\n",
    "    fast_inference=True,  # Enable vLLM fast inference\n",
    "    max_lora_rank=lora_rank,\n",
    "    gpu_memory_utilization=0.85,\n",
    ")\n",
    "\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=lora_rank,  # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "    ],  # Remove QKVO if out of memory\n",
    "    lora_alpha=lora_rank,\n",
    "    use_gradient_checkpointing=\"unsloth\",  # Enable long context finetuning\n",
    "    random_state=3407,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d78cbf",
   "metadata": {},
   "source": [
    "# Pre Train\n",
    "预训练使模型遵循交互式提交的格式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009cc6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "\n",
    "sys_prompt=\"\"\"You are a professional code assistant with expertise in code analysis and version control best practices.\n",
    "1. Your task is to analyze code changes between <DIFF></DIFF>, summarize their theme and core content.\n",
    "2. Generate a structured commit message between <COMMIT></COMMIT>. Your message must strictly follow the conventional commits specification.\n",
    "\"\"\"\n",
    "\n",
    "ds = datasets.load_from_disk(\"/home/circle/code/AImmit_ai/dataset_generation\")\n",
    "ds = ds.to_pandas()[\n",
    "    [\"diff\", \"commit_message\"]\n",
    "]\n",
    "ds = ds[:50]\n",
    "\n",
    "def format_dataset(x):\n",
    "    diff = f\"<DIFF>{x['diff']}</DIFF>\"\n",
    "    commit_message = x[\"commit_message\"]\n",
    "    final_prompt = f\"<COMMIT>{commit_message}</COMMIT>\"\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": sys_prompt},\n",
    "        {\"role\": \"user\", \"content\": diff},\n",
    "        {\"role\": \"assistant\", \"content\": final_prompt},\n",
    "    ]\n",
    "\n",
    "ds[\"prompt\"] = ds.apply(format_dataset, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941aed1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_template = \\\n",
    "    \"{% if messages[0]['role'] == 'system' %}\"\\\n",
    "        \"{{ messages[0]['content'] + eos_token }}\"\\\n",
    "        \"{% set loop_messages = messages[1:] %}\"\\\n",
    "    \"{% else %}\"\\\n",
    "        \"{{ '{system_prompt}' + eos_token }}\"\\\n",
    "        \"{% set loop_messages = messages %}\"\\\n",
    "    \"{% endif %}\"\\\n",
    "    \"{% for message in loop_messages %}\"\\\n",
    "        \"{% if message['role'] == 'user' %}\"\\\n",
    "            \"{{ message['content'] }}\"\\\n",
    "        \"{% elif message['role'] == 'assistant' %}\"\\\n",
    "            \"{{ message['content'] + eos_token }}\"\\\n",
    "        \"{% endif %}\"\\\n",
    "    \"{% endfor %}\"\\\n",
    "    \"{% if add_generation_prompt %}{{ '<COMMIT>' }}\"\\\n",
    "    \"{% endif %}\"\n",
    "chat_template = chat_template\\\n",
    "    .replace(\"'{system_prompt}'\",   f\"'{sys_prompt}'\")\n",
    "tokenizer.chat_template = chat_template\n",
    "\n",
    "tokenizer.apply_chat_template(ds[\"prompt\"][0], tokenize = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6a6f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "ds[\"text\"] = tokenizer.apply_chat_template(ds[\"prompt\"].values.tolist(), tokenize = False)\n",
    "dataset = Dataset.from_pandas(ds)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78de335e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer, SFTConfig\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset,\n",
    "    args = SFTConfig(\n",
    "        dataset_text_field = \"text\",\n",
    "        per_device_train_batch_size = 1,\n",
    "        gradient_accumulation_steps = 1, # Use GA to mimic batch size!\n",
    "        warmup_steps = 5,\n",
    "        num_train_epochs = 2, # Set this for 1 full training run.\n",
    "        learning_rate = 2e-4, # Reduce to 2e-5 for long training runs\n",
    "        logging_steps = 5,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        report_to = \"none\", # Use this for WandB etc\n",
    "    ),\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6332c1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = tokenizer.apply_chat_template(\n",
    "    dataset[0][\"prompt\"][:2],\n",
    "    tokenize = False,\n",
    "    add_generation_prompt = True, # Must add for generation\n",
    ")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "_ = model.generate(\n",
    "    **tokenizer(text, return_tensors = \"pt\").to(\"cuda\"),\n",
    "    temperature = 0,\n",
    "    max_new_tokens = 1024,\n",
    "    streamer = TextStreamer(tokenizer, skip_prompt = False),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519e1a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch\n",
    "del dataset\n",
    "del ds\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b241ce",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "750916d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "\n",
    "sys_prompt=f\"\"\"{sys_prompt}\n",
    "3. Ensure the commit message is concise yet informative.\n",
    "4. Focus on the most significant changes if the diff is extensive.\"\"\"\n",
    "\n",
    "# ds = datasets.load_dataset(\"circle33/conventional_commits\")\n",
    "ds = datasets.load_from_disk(\"/home/circle/code/AImmit_ai/dataset_generation\")\n",
    "\n",
    "ds = ds.map(\n",
    "    lambda x: {\n",
    "        \"prompt\": [\n",
    "            {\"role\": \"system\", \"content\": sys_prompt},\n",
    "            {\"role\": \"user\", \"content\": x[\"diff\"]},\n",
    "        ],\n",
    "        \"commit_message\": x[\"commit_message\"],\n",
    "    }\n",
    ")\n",
    "\n",
    "ds[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3360e035",
   "metadata": {},
   "source": [
    "# Reward functions\n",
    "\n",
    "1. Format reward: ensure the output is in the correct format. (10 points)\n",
    "2. Check the output for similarity to known commit messages. The higher the similarity, the higher the reward. (90 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cda4d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "commit_end_regex = r\"</COMMIT>[\\s]{0,}\" + \\\n",
    "    \"(?:\" + re.escape(tokenizer.eos_token) + \")?\"\n",
    "\n",
    "match_format = re.compile(\n",
    "    rf\"<COMMIT>(.+?){commit_end_regex}\"\\\n",
    "    rf\"[\\s]{{0,}}$\",\n",
    "    flags = re.MULTILINE | re.DOTALL\n",
    ")\n",
    "\n",
    "def format_exactaly_reward(prompts, completions, **kwargs):\n",
    "    responses = [completion[0][\"content\"] for completion in completions]\n",
    "\n",
    "    return [\n",
    "        3.0 if match_format.search(response) is not None else 0 for response in responses\n",
    "    ]\n",
    "\n",
    "def format_approximately_reward(prompts, completions, **kwargs):\n",
    "    scores = []\n",
    "    for completion in completions:\n",
    "        score = 0\n",
    "        response = completion[0][\"content\"]\n",
    "\n",
    "        score += 0.5 if response.count(\"</COMMIT>\") == 1 else -1.0\n",
    "        scores.append(score)\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219a02c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers.cross_encoder import CrossEncoder\n",
    "\n",
    "CROSS_MODEL = CrossEncoder(\"cross-encoder/stsb-distilroberta-base\")\n",
    "\n",
    "def cross_compare(model_commit, input_commit):\n",
    "    \"\"\"\n",
    "    Compares two commit messages using a cross-encoder model and returns a similarity score.\n",
    "    The score is normalized to a 0-scale.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        score = CROSS_MODEL.predict([model_commit, input_commit])\n",
    "        if score > 0.9:\n",
    "            return 2.0\n",
    "        elif score > 0.7:\n",
    "            return 1.5\n",
    "        elif score > 0.5:\n",
    "            return 1.0\n",
    "        else:\n",
    "            return 0.0\n",
    "    except Exception as e:\n",
    "        print(f\"Error during semantic similarity calculation: {e}\")\n",
    "        return 0.0\n",
    "\n",
    "match_commit = re.compile(\n",
    "    r\"<COMMIT>(.*?)</COMMIT>\",\n",
    "    flags = re.MULTILINE | re.DOTALL\n",
    ")\n",
    "\n",
    "global PRINTED_TIMES\n",
    "global PRINT_EVERY_STEPS\n",
    "PRINTED_TIMES = 0\n",
    "PRINT_EVERY_STEPS = 5\n",
    "\n",
    "def score_reward(prompts, completions, commit_message, diff, **kwargs):\n",
    "    diff_input = prompts[0][-1][\"content\"]\n",
    "    responses = [completion[0][\"content\"] for completion in completions]\n",
    "    extracted_responses = [\n",
    "        guess.group(1)\n",
    "        if (guess := match_commit.search(r)) is not None else None \\\n",
    "        for r in responses\n",
    "    ]\n",
    "    scores = []\n",
    "\n",
    "    global PRINTED_TIMES\n",
    "    global PRINT_EVERY_STEPS\n",
    "    if PRINTED_TIMES % PRINT_EVERY_STEPS == 0:\n",
    "        print(\n",
    "            '='*50 + f\"DIFF_INPUT:\\n{diff_input}\", f\"\\nEXPECTED_COMMIT:\\n{commit_message[0]}\", f\"\\nMODEL_RESPONSE:\\n{responses[0]}\"\n",
    "        )\n",
    "    PRINTED_TIMES += 1\n",
    "\n",
    "    for guess, input_commit in zip(extracted_responses, commit_message):\n",
    "        if guess is None:\n",
    "            scores.append(-2.5)\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            scores.append(cross_compare(model_commit=guess, input_commit=input_commit))\n",
    "        except:\n",
    "            scores.append(0.0)\n",
    "            continue\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b5c2e9",
   "metadata": {},
   "source": [
    "# Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69819690",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized = ds.map(\n",
    "    lambda x: {\"tokens\" : tokenizer.apply_chat_template(x[\"prompt\"], add_generation_prompt = True, tokenize = True)},\n",
    "    batched = True,\n",
    ")\n",
    "print(tokenizer.decode(tokenized[0][\"tokens\"]))\n",
    "tokenized = tokenized.map(lambda x: {\"L\" : len(x[\"tokens\"])})\n",
    "\n",
    "import numpy as np\n",
    "maximun_length = int(np.quantile(tokenized[\"L\"], 0.9))\n",
    "print(\"Max Length = \", maximun_length)\n",
    "\n",
    "# ds = ds.select(np.where(np.array(tokenized[\"L\"]) <= maximun_length)[0])\n",
    "del tokenized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49776164",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d1ff45",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "max_prompt_length = maximun_length + 1\n",
    "max_completion_length = max_seq_length - max_prompt_length\n",
    "new_model_id=\"circle33/qwen-commit-7b-grpo\"\n",
    "\n",
    "from vllm import SamplingParams\n",
    "vllm_sampling_params = SamplingParams(\n",
    "    min_p = 0.1,\n",
    "    top_p = 1.0,\n",
    "    top_k = -1,\n",
    "    seed = 3407,\n",
    "    stop = [tokenizer.eos_token],\n",
    "    include_stop_str_in_output = True,\n",
    ")\n",
    "\n",
    "from trl import GRPOConfig, GRPOTrainer\n",
    "training_args = GRPOConfig(\n",
    "    vllm_sampling_params = vllm_sampling_params,\n",
    "    learning_rate = 8e-6,\n",
    "    adam_beta1 = 0.9,\n",
    "    adam_beta2 = 0.99,\n",
    "    weight_decay = 0.1,\n",
    "    warmup_ratio = 0.01,\n",
    "    lr_scheduler_type = \"cosine\",\n",
    "    optim = \"paged_adamw_8bit\",\n",
    "    logging_steps = 1,\n",
    "    per_device_train_batch_size = 4,\n",
    "    gradient_accumulation_steps = 1,\n",
    "    num_generations = 2, # Decrease if out of memory\n",
    "    max_prompt_length = max_prompt_length,\n",
    "    max_completion_length = max_completion_length,\n",
    "    max_grad_norm = 0.1,\n",
    "    output_dir = \"outputs\",\n",
    "    overwrite_output_dir = True,\n",
    "    push_to_hub = True,\n",
    "    hub_model_id=new_model_id,\n",
    "    hub_strategy=\"every_save\",\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=50,\n",
    "    save_total_limit=1,\n",
    "    num_train_epochs=3,\n",
    "    unsloth_num_chunks=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48bd20f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.init(project=\"GRPO-reboost\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fbf962d",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = GRPOTrainer(\n",
    "    model = model,\n",
    "    processing_class = tokenizer,\n",
    "    reward_funcs=[\n",
    "        format_exactaly_reward,\n",
    "        format_approximately_reward,\n",
    "        score_reward,\n",
    "    ],\n",
    "    args = training_args,\n",
    "    train_dataset = ds,\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AImmit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
